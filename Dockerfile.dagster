FROM python:3.10-slim

WORKDIR /app

# Install system dependencies (Java for Spark client)
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    wget \
    netcat-openbsd \
    openjdk-17-jdk \
    && rm -rf /var/lib/apt/lists/*

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Copy Dagster requirements
COPY requirements-dagster.txt .
RUN pip install --no-cache-dir -r requirements-dagster.txt

# Copy application code
COPY crypto_pipeline/ ./crypto_pipeline/
COPY shared/ ./shared/
COPY config/ ./config/

# Create directories
RUN mkdir -p /app/dagster_home /app/logs /app/spark/jars

# Download required Spark jars
RUN echo "Downloading Hudi Spark bundle..." && \
    wget -O /app/spark/jars/hudi-spark3.4-bundle_2.12-0.14.0.jar \
    https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.4-bundle_2.12/0.14.0/hudi-spark3.4-bundle_2.12-0.14.0.jar && \
    echo "Downloading AWS SDK..." && \
    wget -O /app/spark/jars/aws-java-sdk-bundle-1.12.367.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar && \
    echo "Downloading Hadoop AWS..." && \
    wget -O /app/spark/jars/hadoop-aws-3.3.4.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    echo "Downloading Kafka Spark SQL..." && \
    wget -O /app/spark/jars/spark-sql-kafka-0-10_2.12-3.4.3.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.3/spark-sql-kafka-0-10_2.12-3.4.3.jar && \
    echo "Downloading Kafka clients..." && \
    wget -O /app/spark/jars/kafka-clients-3.4.1.jar \
    https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar && \
    echo "Downloading Spark token provider..." && \
    wget -O /app/spark/jars/spark-token-provider-kafka-0-10_2.12-3.4.3.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.4.3/spark-token-provider-kafka-0-10_2.12-3.4.3.jar && \
    echo "All Spark jars downloaded successfully!"

# Create app user
RUN groupadd -r dagster && useradd -r -g dagster dagster
RUN chown -R dagster:dagster /app
USER dagster

# Environment
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV DAGSTER_HOME=/app/dagster_home

# Health check for webserver
HEALTHCHECK --interval=60s --timeout=30s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:3000/server_info || exit 1

# Default command
CMD ["dagster-webserver", "-h", "0.0.0.0", "-p", "3000"]